<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title> Prompt Tuning â€” The Power of Scale for Parameter-Efficient Prompt Tuning  | harheem</title>
<meta name="description" content="A simple, minimal Jekyll theme for a personal web page and blog, focusing on white space and readability
">
<meta name="keywords" content="">
<link rel="canonical" href="/llm/2024/02/19/prompt-tuning.html">
        <link rel="icon" type="image/jpeg" href="/assets/img/pudhina.jpg"/>
<link rel="stylesheet" href="/assets/vendor/normalize-css/normalize.css">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="stylesheet" href="/assets/vendor/highlight/styles/agate.css">

<link rel="stylesheet" href="/assets/vendor/font-awesome/css/font-awesome.css">
<link href="https://fonts.googleapis.com/css?family=Quicksand" rel="stylesheet">
    </head>
    <body>
        <div id="section-nav">
            <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#prompt-tuningthe-power-of-scale-for-parameter-efficient-prompt-tuning">
Prompt Tuning
The Power of Scale for Parameter-Efficient Prompt Tuning
</a></li>
<li class="toc-entry toc-h1"><a href="#prompt-tuning">Prompt Tuning</a></li>
<li class="toc-entry toc-h1"><a href="#í”„ë¡¬í”„íŠ¸-íŠœë‹ì´ë€">í”„ë¡¬í”„íŠ¸ íŠœë‹ì´ë€?</a></li>
<li class="toc-entry toc-h1"><a href="#ì‘ë™-ì›ë¦¬">ì‘ë™ ì›ë¦¬</a></li>
<li class="toc-entry toc-h1"><a href="#ê°„ë‹¨í•œ-ì½”ë“œ">ê°„ë‹¨í•œ ì½”ë“œ</a></li>
<li class="toc-entry toc-h1"><a href="#peft-transformers-ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼-í™œìš©í•œ-ì˜ˆì‹œ">peft, transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•œ ì˜ˆì‹œ</a></li>
<li class="toc-entry toc-h1"><a href="#ë§ˆë¬´ë¦¬">ë§ˆë¬´ë¦¬</a></li>
<li class="toc-entry toc-h1"><a href="#reference">Reference</a></li>
</ul>
        </div>
        <div class="wrapper">
            <header class="header">
<div class="navigation">
<a href="/" class="logo">harheem</a>
<ul class="menu">
<li class="menu__entry"><a href="/resume">Resume</a></li>
<li class="menu__entry"><a href="/blog">Blog</a></li>
</ul>
</div>
<ul class="social-links">

<a href="mailto:shhr.kre@gmail.com" class="social-links__entry" target="_blank">
<i class="fa fa-envelope-square"></i>
</a>


<a href="https://github.com/harheem" class="social-links__entry" target="_blank">
<i class="fa fa-github"></i>
</a>


<a href="https://in.linkedin.com/in/harheemk" class="social-links__entry" target="_blank">
<i class="fa fa-linkedin"></i>
</a>

</ul>
</header>
            <h1 class="page-title">
<a class="anchor" href="#prompt-tuningthe-power-of-scale-for-parameter-efficient-prompt-tuning" aria-hidden="true"><span class="octicon octicon-link"></span></a>
<div class="page-title__text">Prompt Tuning</div>
<div class="page-title__subtitle">The Power of Scale for Parameter-Efficient Prompt Tuning</div>
</h1>

<h1 id="prompt-tuning">
<a class="anchor" href="#prompt-tuning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Prompt Tuning</h1>

<blockquote>
  <p><strong>PEFT(Parameter-Efficient Fine-Tuning)ëŠ”Â ì ì€Â ìˆ˜ì˜Â íŒŒë¼ë¯¸í„°ë¥¼Â í•™ìŠµí•˜ëŠ”ê²ƒë§Œìœ¼ë¡œÂ ëª¨ë¸Â ì „ì²´ë¥¼Â íŒŒì¸íŠœë‹í•˜ëŠ”Â ê²ƒê³¼Â ìœ ì‚¬í•œÂ íš¨ê³¼ë¥¼Â ëˆ„ë¦´Â ìˆ˜Â ìˆë„ë¡Â í•´ì¤ë‹ˆë‹¤. PEFT ë°©ë²• ì¤‘ í•˜ë‚˜ì¸ Prompt Tuningì— ëŒ€í•´ì„œ ì•Œì•„ë´…ì‹œë‹¤.</strong></p>

</blockquote>

<p><a href="https://arxiv.org/pdf/2104.08691.pdf">https://arxiv.org/pdf/2104.08691.pdf</a></p>

<h1 id="í”„ë¡¬í”„íŠ¸-íŠœë‹ì´ë€">
<a class="anchor" href="#%ED%94%84%EB%A1%AC%ED%94%84%ED%8A%B8-%ED%8A%9C%EB%8B%9D%EC%9D%B4%EB%9E%80" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>í”„ë¡¬í”„íŠ¸ íŠœë‹ì´ë€?</strong>
</h1>

<p>ì–¸ì–´ ëª¨ë¸ì„ íŠ¹ì • ì‘ì—…ì— ë§ê²Œ ì¡°ì •í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë°©ì‹ì€ ëª¨ë¸ì„ íŠ¹ì • ì‘ì—…ì— ë§ê²Œ ì „ì²´ì ìœ¼ë¡œ ì¡°ì •í•´ì•¼ í–ˆì§€ë§Œ, í”„ë¡¬í”„íŠ¸ íŠœë‹ì€ ëª¨ë¸ì˜ í•µì‹¬ ë¶€ë¶„ì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ì„œ ì‘ì—… íŠ¹í™” ë¶€ë¶„ë§Œ ì¡°ì •í•©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì˜ â€˜ëƒ‰ë™â€™(frozen) ìƒíƒœë¥¼ ìœ ì§€í•˜ë©´ì„œë„ í•„ìš”í•œ ë¶€ë¶„ì—ë§Œ ì´ˆì ì„ ë§ì¶”ì–´ íš¨ìœ¨ì„±ì„ ë†’ì´ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.</p>

<p><img src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2a330106-7d16-49d5-9057-343dfb0cb92c%2F06fcee26-22da-429a-9e1b-46c98535ed1d%2FUntitled.png?table=block&amp;id=1be7483c-133f-42ec-b874-da2bdfce41bb&amp;spaceId=2a330106-7d16-49d5-9057-343dfb0cb92c&amp;width=2000&amp;userId=93a922d0-0a24-445b-bddf-8a085b93d655&amp;cache=v2" alt="*Prompt tuning retains the strong task performance of model tuning, while keeping the pre-trained model frozen, enabling efficient multitask serving.*"></p>

<p><em>Prompt tuning retains the strong task performance of model tuning, while keeping the pre-trained model frozen, enabling efficient multitask serving.</em></p>

<h1 id="ì‘ë™-ì›ë¦¬">
<a class="anchor" href="#%EC%9E%91%EB%8F%99-%EC%9B%90%EB%A6%AC" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>ì‘ë™ ì›ë¦¬</strong>
</h1>

<p>ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ëŠ” í•™ìŠµ ê°€ëŠ¥í•œ â€˜ë²¡í„°â€™ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤. ì´ ë²¡í„°ë“¤ì€ ì…ë ¥ í…ìŠ¤íŠ¸ì™€ ê²°í•©ë˜ì–´ ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ë²¡í„°ë“¤ì€ ê¸°ì¡´ ì–´íœ˜ì— ì†í•˜ì§€ ì•ŠëŠ” â€˜ê°€ìƒì˜ í† í°(virtual tokens)â€™ìœ¼ë¡œì„œ ì‘ë™í•˜ë©°, ëª¨ë¸ì˜ ê¸°ì¡´ íŒŒë¼ë¯¸í„°ë¥¼ ë³€ê²½í•˜ì§€ ì•Šê³ ë„ íŠ¹ì • ì‘ì—…ì— ëŒ€í•œ ëª¨ë¸ì˜ ë°˜ì‘ì„ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì€ ì´ ì…ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³ , ì´ ê³¼ì •ì—ì„œ ì˜¤ì°¨ë¥¼ ê³„ì‚°í•˜ì—¬ ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì„ í†µí•´, ë‹¤ì–‘í•œ ì‘ì—…ì— ëŒ€í•œ ì§€ì‹ì„ íš¨ê³¼ì ìœ¼ë¡œ í¡ìˆ˜í•˜ê³  ì ìš©í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.</p>

<p><img src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2a330106-7d16-49d5-9057-343dfb0cb92c%2F44052b28-0850-4572-a0dc-2d28dbaa6dcd%2FUntitled.png?table=block&amp;id=8de0b385-4ab8-4411-a713-0b03e716c3a0&amp;spaceId=2a330106-7d16-49d5-9057-343dfb0cb92c&amp;width=2000&amp;userId=93a922d0-0a24-445b-bddf-8a085b93d655&amp;cache=v2" alt="Untitled"></p>

<p>ë¨¼ì €, ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ê³ ì • ê¸¸ì´(e.g., 20 tokens long)ì˜ ë²¡í„° ì‹œí€€ìŠ¤ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. ì´ ë²¡í„°ë“¤ì€ ëª¨ë¸ì˜ ì…ë ¥ í…ìŠ¤íŠ¸ ì•ì— ë°°ì¹˜ë©ë‹ˆë‹¤.</p>

<p>ëª¨ë¸ì´ ì…ë ¥ì„ ì²˜ë¦¬í•  ë•Œ, ì´ ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ ë²¡í„°ë“¤ë„ í•¨ê»˜ ì²˜ë¦¬ë©ë‹ˆë‹¤. ëª¨ë¸ì´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ë©´, ì˜ˆì¸¡ ê²°ê³¼ì™€ ì‹¤ì œ íƒ€ê²Ÿ ê°„ì˜ ì˜¤ì°¨ë¥¼ ê³„ì‚°í•˜ì—¬ ì´ ì˜¤ì°¨ë¥¼ ì‚¬ìš©í•´ ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ ë²¡í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.</p>

<h1 id="ê°„ë‹¨í•œ-ì½”ë“œ">
<a class="anchor" href="#%EA%B0%84%EB%8B%A8%ED%95%9C-%EC%BD%94%EB%93%9C" aria-hidden="true"><span class="octicon octicon-link"></span></a>ê°„ë‹¨í•œ ì½”ë“œ</h1>

<hr>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">SoftEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> 
                <span class="n">wte</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">,</span>
                <span class="n">n_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> 
                <span class="n">random_range</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
                <span class="n">initialize_from_vocab</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">appends learned embedding to 

        Args:
            wte (nn.Embedding): original transformer word embedding
            n_tokens (int, optional): number of tokens for task. Defaults to 10.
            random_range (float, optional): range to init embedding (if not initialize from vocab). Defaults to 0.5.
            initialize_from_vocab (bool, optional): initalizes from default vocab. Defaults to True.
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">SoftEmbedding</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">wte</span> <span class="o">=</span> <span class="n">wte</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_tokens</span> <span class="o">=</span> <span class="n">n_tokens</span>
        <span class="n">self</span><span class="p">.</span><span class="n">learned_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">parameter</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">initialize_embedding</span><span class="p">(</span><span class="n">wte</span><span class="p">,</span>
                                                                               <span class="n">n_tokens</span><span class="p">,</span> 
                                                                               <span class="n">random_range</span><span class="p">,</span> 
                                                                               <span class="n">initialize_from_vocab</span><span class="p">))</span>
            
    <span class="k">def</span> <span class="nf">initialize_embedding</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> 
                             <span class="n">wte</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">,</span>
                             <span class="n">n_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> 
                             <span class="n">random_range</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> 
                             <span class="n">initialize_from_vocab</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">initializes learned embedding

        Args:
            same as __init__

        Returns:
            torch.float: initialized using original schemes
        </span><span class="sh">"""</span>
        <span class="k">if</span> <span class="n">initialize_from_vocab</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">wte</span><span class="p">.</span><span class="n">weight</span><span class="p">[:</span><span class="n">n_tokens</span><span class="p">].</span><span class="nf">clone</span><span class="p">().</span><span class="nf">detach</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">n_tokens</span><span class="p">,</span> <span class="n">wte</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)).</span><span class="nf">uniform_</span><span class="p">(</span><span class="o">-</span><span class="n">random_range</span><span class="p">,</span> <span class="n">random_range</span><span class="p">)</span>
            
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">run forward pass

        Args:
            tokens (torch.long): input tokens before encoding

        Returns:
            torch.float: encoding of text concatenated with learned task specifc embedding
        </span><span class="sh">"""</span>

				<span class="c1"># Changes: Apply word embeddings to the entire set of input tokens without slicing
</span>        <span class="n">input_embedding</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">wte</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
        <span class="n">learned_embedding</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">learned_embedding</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="n">input_embedding</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">learned_embedding</span><span class="p">,</span> <span class="n">input_embedding</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>ì¶œì²˜: <a href="https://github.com/kipgparker/soft-prompt-tuning/blob/main/soft_embedding.py">https://github.com/kipgparker/soft-prompt-tuning</a></p>

<p>ì´ ì½”ë“œëŠ” PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ â€˜SoftEmbeddingâ€™ì´ë¼ëŠ” ì‹ ê²½ë§ ëª¨ë“ˆì„ ì •ì˜í•©ë‹ˆë‹¤.</p>

<p>ì´ ëª¨ë“ˆì˜ ì£¼ìš” ëª©ì ì€ ê¸°ì¡´ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ì›Œë“œ ì„ë² ë”©(word embedding)ì— ì¶”ê°€ì ì¸ í•™ìŠµ ê°€ëŠ¥í•œ ì„ë² ë”©ì„ ê²°í•©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ íŠ¹ì • ì‘ì—…ì— ëŒ€í•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<p>ì¶œì²˜ ì½”ë“œì—ì„œëŠ” <code class="language-plaintext highlighter-rouge">input_embedding = self.wte(tokens[:, self.n_tokens:])</code> ë¡œ ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ í† í°ì˜ ê¸¸ì´ë§Œí¼ ì›ë³¸ ì„ë² ë”©ì„ ì˜ë¼ì„œ ê²°í•©í•˜ì˜€ì§€ë§Œ, ì €ëŠ” ì›ë³¸ ì„ë² ë”©ì— ì¶”ê°€ëœ ì„ë² ë”©ì„ ê²°í•©í•´ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´  ë‹¤ìŒê³¼ ê°™ì´ ì½”ë“œë¥¼ ë³€ê²½í•˜ì˜€ìŠµë‹ˆë‹¤: <code class="language-plaintext highlighter-rouge">input_embedding = self.wte(tokens)</code></p>

<p>ì½”ë“œë¥¼ ìì„¸íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">**SoftEmbedding**</code></p>

<p>ì´ í´ë˜ìŠ¤ëŠ” <code class="language-plaintext highlighter-rouge">nn.Module</code>ì„ ìƒì†ë°›ì•„ PyTorchì˜ ì‹ ê²½ë§ ëª¨ë“ˆë¡œ ì •ì˜ë©ë‹ˆë‹¤.</p>

<p><code class="language-plaintext highlighter-rouge">**__init__**</code></p>

<ul>
  <li>
<code class="language-plaintext highlighter-rouge">wte (nn.Embedding)</code>: ê¸°ì¡´ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ì›Œë“œ ì„ë² ë”©ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.</li>
  <li>
<code class="language-plaintext highlighter-rouge">n_tokens (int)</code>: í•™ìŠµ ê°€ëŠ¥í•œ ì¶”ê°€ í† í°ì˜ ìˆ˜ì…ë‹ˆë‹¤. ì´ ê°’ì´ 10ì¼ ë•Œ, 10ê°œì˜ ì¶”ê°€ ì„ë² ë”© í† í°ì´ ìƒì„±ë©ë‹ˆë‹¤.</li>
  <li>
<code class="language-plaintext highlighter-rouge">random_range (float)</code>: ì„ë² ë”©ì„ ì´ˆê¸°í™”í•  ë•Œ ì‚¬ìš©ë˜ëŠ” ë²”ìœ„ì…ë‹ˆë‹¤. ì´ ê°’ì´ 0.5ì¼ ë•Œ, ê° ì„ë² ë”© ê°’ì€ -0.5 ~ 0.5 ì‚¬ì´ì˜ ë²”ìœ„ì—ì„œ ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤.</li>
  <li>
<code class="language-plaintext highlighter-rouge">initialize_from_vocab (bool)</code>: ê¸°ì¡´ ì–´íœ˜ì—ì„œ ì„ë² ë”©ì„ ì´ˆê¸°í™”í• ì§€ ì—¬ë¶€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤. ì´ ê°’ì€ ì•„ë˜ <code class="language-plaintext highlighter-rouge">initialize_embedding</code> ì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ëŠ”ì§€ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
  <li>
<code class="language-plaintext highlighter-rouge">learned_embedding</code>: íŠ¹ì • ì‘ì—…ì— íŠ¹í™”ëœ ì •ë³´ë¥¼ í¬í•¨í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ëœ ìƒˆë¡œìš´ ì„ë² ë”©ì…ë‹ˆë‹¤. ì¶”ê°€ì ì¸ í•™ìŠµ ê°€ëŠ¥í•œ ì„ë² ë”©ì„ ì •ì˜í•˜ë©°, ì´ˆê¸°í™” ë°©ë²•ì€ <code class="language-plaintext highlighter-rouge">initialize_embedding</code> ë©”ì„œë“œì— ì˜í•´ ê²°ì •ë©ë‹ˆë‹¤.</li>
</ul>

<p><strong><code class="language-plaintext highlighter-rouge">initialize_embedding</code></strong></p>

<ul>
  <li>ì´ ë©”ì„œë“œëŠ” ì¶”ê°€ ì„ë² ë”©ì„ ì´ˆê¸°í™”í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.</li>
  <li>
<code class="language-plaintext highlighter-rouge">initialize_from_vocab</code>ê°€ <code class="language-plaintext highlighter-rouge">True</code>ì´ë©´ ê¸°ì¡´ì˜ ì›Œë“œ ì„ë² ë”©(wte)ì—ì„œ ì²˜ìŒ <code class="language-plaintext highlighter-rouge">n_tokens</code>ë§Œí¼ì„ ë³µì‚¬í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ê¸°ì¡´ ì–´íœ˜ì— ê¸°ë°˜í•œ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—, ëª¨ë¸ì´ ì´ë¯¸ í•™ìŠµí•œ ì–¸ì–´ì  íŠ¹ì„±ì„ ìœ ì§€í•˜ë„ë¡ í•©ë‹ˆë‹¤.</li>
  <li>
<code class="language-plaintext highlighter-rouge">False</code>ì¸ ê²½ìš°, ì§€ì •ëœ <code class="language-plaintext highlighter-rouge">random_range</code>ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”©ì„ ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ëª¨ë¸ì´ ì´ì „ì— ë³´ì§€ ëª»í•œ ìƒˆë¡œìš´ ì¢…ë¥˜ì˜ ë°ì´í„°ë‚˜ ì‘ì—…ì— ëŒ€ì‘í•´ì•¼ í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.</li>
</ul>

<p><code class="language-plaintext highlighter-rouge">**forward**</code></p>

<ul>
  <li>ëª¨ë¸ì´ ì…ë ¥ ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ëŠ”ì§€ ì •ì˜í•©ë‹ˆë‹¤. ì´ ë©”ì„œë“œëŠ” ì…ë ¥ í† í°ì„ ë°›ì•„ ì¶”ê°€ì ì¸ í•™ìŠµëœ ì„ë² ë”©ê³¼ í•¨ê»˜ ì›ë˜ì˜ ì›Œë“œ ì„ë² ë”©ì„ ê²°í•©í•©ë‹ˆë‹¤.</li>
  <li>
<code class="language-plaintext highlighter-rouge">tokens</code>: ì…ë ¥ ë°ì´í„°ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ ì²˜ë¦¬í•  ì›ì‹œ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•œ ê²ƒì…ë‹ˆë‹¤.</li>
  <li>
<code class="language-plaintext highlighter-rouge">learned_embedding</code>ì€ ëª¨ë“  ì…ë ¥ì— ëŒ€í•´ ë°˜ë³µë˜ë©°, ê¸°ì¡´ ì…ë ¥ ì„ë² ë”©ê³¼ ì—°ê²°ë©ë‹ˆë‹¤.</li>
  <li>ìµœì¢…ì ìœ¼ë¡œ, í•™ìŠµëœ ì„ë² ë”©ê³¼ ì…ë ¥ ì„ë² ë”©ì´ ì—°ê²°ë˜ì–´ ë°˜í™˜ë©ë‹ˆë‹¤.</li>
</ul>

<h1 id="peft-transformers-ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼-í™œìš©í•œ-ì˜ˆì‹œ">
<a class="anchor" href="#peft-transformers-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%EC%98%88%EC%8B%9C" aria-hidden="true"><span class="octicon octicon-link"></span></a>peft, transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•œ ì˜ˆì‹œ</h1>

<hr>

<p>ì‹œì‘í•˜ê¸° ì „ì— <strong><code class="language-plaintext highlighter-rouge">peft</code></strong>, <strong><code class="language-plaintext highlighter-rouge">transformers</code></strong>, <strong><code class="language-plaintext highlighter-rouge">datasets</code>, <code class="language-plaintext highlighter-rouge">torch</code></strong> ë“± í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">peft</span> <span class="n">transformers</span> <span class="n">datasets</span> <span class="n">torch</span>
</code></pre></div></div>

<p>ì‚¬ìš©í•  ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ì´ ì˜ˆì‹œì—ì„œëŠ” <strong><code class="language-plaintext highlighter-rouge">bigscience/bloomz-560m</code></strong>ì„ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¡œ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. <strong><code class="language-plaintext highlighter-rouge">PromptTuningConfig</code></strong>ë¥¼ ì •ì˜í•˜ì—¬ ì‘ì—… ìœ í˜•, ê°€ìƒ í† í°ì˜ ìˆ˜, ì´ˆê¸°í™” í…ìŠ¤íŠ¸, í† í¬ë‚˜ì´ì € ì´ë¦„ ë˜ëŠ” ê²½ë¡œ ë“±ì˜ ì„¸ë¶€ ì •ë³´ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">from</span> <span class="n">peft</span> <span class="kn">import</span> <span class="n">get_peft_config</span><span class="p">,</span> <span class="n">get_peft_model</span><span class="p">,</span> <span class="n">PromptTuningInit</span><span class="p">,</span> <span class="n">PromptTuningConfig</span><span class="p">,</span> <span class="n">TaskType</span><span class="p">,</span> <span class="n">PeftType</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">default_data_collator</span><span class="p">,</span> <span class="n">get_linear_schedule_with_warmup</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">device</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span>
<span class="n">model_name_or_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">bigscience/bloomz-560m</span><span class="sh">"</span>
<span class="n">tokenizer_name_or_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">bigscience/bloomz-560m</span><span class="sh">"</span>
<span class="n">peft_config</span> <span class="o">=</span> <span class="nc">PromptTuningConfig</span><span class="p">(</span>
    <span class="n">task_type</span><span class="o">=</span><span class="n">TaskType</span><span class="p">.</span><span class="n">CAUSAL_LM</span><span class="p">,</span>
    <span class="n">prompt_tuning_init</span><span class="o">=</span><span class="n">PromptTuningInit</span><span class="p">.</span><span class="n">TEXT</span><span class="p">,</span>
    <span class="n">num_virtual_tokens</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">prompt_tuning_init_text</span><span class="o">=</span><span class="sh">"</span><span class="s">Classify if the tweet is a complaint or not:</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">tokenizer_name_or_path</span><span class="o">=</span><span class="n">model_name_or_path</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">dataset_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">twitter_complaints</span><span class="sh">"</span>
<span class="n">checkpoint_name</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">dataset_name</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">model_name_or_path</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">peft_config</span><span class="p">.</span><span class="n">peft_type</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">peft_config</span><span class="p">.</span><span class="n">task_type</span><span class="si">}</span><span class="s">_v1.pt</span><span class="sh">"</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">/</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">_</span><span class="sh">"</span>
<span class="p">)</span>
<span class="n">text_column</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Tweet text</span><span class="sh">"</span>
<span class="n">label_column</span> <span class="o">=</span> <span class="sh">"</span><span class="s">text_label</span><span class="sh">"</span>
<span class="n">max_length</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">3e-2</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">8</span>
</code></pre></div></div>

<p>ì´ ì˜ˆì œì—ì„œëŠ” <strong><code class="language-plaintext highlighter-rouge">ought/raft</code></strong>ì˜ <strong><code class="language-plaintext highlighter-rouge">twitter_complaints</code></strong>ë¼ëŠ” ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ íŠ¸ìœ„í„°ì˜ íŠ¸ìœ—ë“¤ì„ í¬í•¨í•˜ê³  ìˆìœ¼ë©°, ê°ì • ë¶„ì„ì´ë‚˜ í…ìŠ¤íŠ¸ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì—°êµ¬ì— ì£¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. ë°ì´í„°ì…‹ì„ ì „ì²˜ë¦¬í•˜ëŠ” ì½”ë“œëŠ” ìƒëµí•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ì°¸ê³  ì½”ë“œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">ought/raft</span><span class="sh">"</span><span class="p">,</span> <span class="n">dataset_name</span><span class="p">)</span>

<span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">"</span><span class="s">_</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">].</span><span class="n">features</span><span class="p">[</span><span class="sh">"</span><span class="s">Label</span><span class="sh">"</span><span class="p">].</span><span class="n">names</span><span class="p">]</span>
<span class="nf">print</span><span class="p">(</span><span class="n">classes</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">{</span><span class="sh">"</span><span class="s">text_label</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="n">classes</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">x</span><span class="p">[</span><span class="sh">"</span><span class="s">Label</span><span class="sh">"</span><span class="p">]]},</span>
    <span class="n">batched</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">num_proc</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="n">dataset</span><span class="p">[</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<p>ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. <code class="language-plaintext highlighter-rouge">print_trainable_parameters()</code>ë¡œ í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë“¤ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># creating model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="nf">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">peft_config</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">print_trainable_parameters</span><span class="p">()</span>
<span class="n">model</span>
</code></pre></div></div>

<p>ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ìµœì í™”í•˜ê¸° ìœ„í•´ AdamW ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. í•™ìŠµë¥ (lr)ë¡œ í•™ìŠµ ê³¼ì •ì—ì„œ ì–¼ë§ˆë‚˜ í° ë‹¨ê³„ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í• ì§€ ê²°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># optimizer and lr scheduler
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="n">lr_scheduler</span> <span class="o">=</span> <span class="nf">get_linear_schedule_with_warmup</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">num_warmup_steps</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">num_training_steps</span><span class="o">=</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_epochs</span><span class="p">),</span>
<span class="p">)</span>
</code></pre></div></div>

<p>ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•´ í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. í›ˆë ¨ëœ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í™•ì¸í•˜ê¸° ìœ„í•´ lossì™€ perplexityë¥¼ í™•ì¸í•©ë‹ˆë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># training and evaluation
</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">tqdm</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)):</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>
        <span class="c1">#         print(batch)
</span>        <span class="c1">#         print(batch["input_ids"].shape)
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">loss</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">float</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
        <span class="n">lr_scheduler</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="n">eval_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">eval_preds</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">tqdm</span><span class="p">(</span><span class="n">eval_dataloader</span><span class="p">)):</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">loss</span>
        <span class="n">eval_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">float</span><span class="p">()</span>
        <span class="n">eval_preds</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span>
            <span class="n">tokenizer</span><span class="p">.</span><span class="nf">batch_decode</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">outputs</span><span class="p">.</span><span class="n">logits</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="n">eval_epoch_loss</span> <span class="o">=</span> <span class="n">eval_loss</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">eval_dataloader</span><span class="p">)</span>
    <span class="n">eval_ppl</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">eval_epoch_loss</span><span class="p">)</span>
    <span class="n">train_epoch_loss</span> <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span>
    <span class="n">train_ppl</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">train_epoch_loss</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">epoch</span><span class="o">=</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">train_ppl</span><span class="o">=</span><span class="si">}</span><span class="s"> </span><span class="si">{</span><span class="n">train_epoch_loss</span><span class="o">=</span><span class="si">}</span><span class="s"> </span><span class="si">{</span><span class="n">eval_ppl</span><span class="o">=</span><span class="si">}</span><span class="s"> </span><span class="si">{</span><span class="n">eval_epoch_loss</span><span class="o">=</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p>ì°¸ê³  ì½”ë“œ: <a href="https://github.com/huggingface/peft/blob/main/examples/causal_language_modeling/peft_prompt_tuning_clm.ipynb">https://github.com/huggingface/peft/blob/main/examples/causal_language_modeling/peft_prompt_tuning_clm.ipynb</a></p>

<p>ë¬¸ì„œ: <a href="https://huggingface.co/docs/peft/v0.8.2/en/package_reference/prompt_tuning#peft.PromptTuningConfig">https://huggingface.co/docs/peft/v0.8.2/en/package_reference/prompt_tuning#peft.PromptTuningConfig</a></p>

<h1 id="ë§ˆë¬´ë¦¬">
<a class="anchor" href="#%EB%A7%88%EB%AC%B4%EB%A6%AC" aria-hidden="true"><span class="octicon octicon-link"></span></a>ë§ˆë¬´ë¦¬</h1>

<hr>

<p>í”„ë¡¬í”„íŠ¸ íŠœë‹ì˜ ê°€ì¥ í° ì¥ì ì€ íš¨ìœ¨ì„±ì…ë‹ˆë‹¤. ì „ì²´ ëª¨ë¸ì„ ë‹¤ì‹œ í•™ìŠµí•˜ì§€ ì•Šê³ ë„, ë§¤ìš° ì ì€ ì–‘ì˜ íŒŒë¼ë¯¸í„°ë§Œìœ¼ë¡œë„ íŠ¹ì • ì‘ì—…ì— ëŒ€í•´ ë†’ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ë‹¤ì–‘í•œ ì‘ì—…ì— í•˜ë‚˜ì˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¦¬ì†ŒìŠ¤ í™œìš©ë„ê°€ ë†’ì•„ì§‘ë‹ˆë‹¤. íŠ¹íˆ ëŒ€ê·œëª¨ ëª¨ë¸ì—ì„œ ì´ ë°©ë²•ì˜ íš¨ê³¼ê°€ í¬ê²Œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.</p>

<p>êµ¬ê¸€ ì—°êµ¬íŒ€ì˜ ë¸”ë¡œê·¸ì— ë”°ë¥´ë©´, í”„ë¡¬í”„íŠ¸ íŠœë‹ì„ ì ìš©í•œ ëª¨ë¸ì€ íŠ¹ì • ë„ë©”ì¸ì˜ ë°ì´í„°ë¡œ í•™ìŠµí•œ í›„, ê´€ë ¨ëœ ë‹¤ë¥¸ ë„ë©”ì¸ì˜ ì‘ì—…ì— ëŒ€í•´ â€˜ì œë¡œ-ìƒ·â€™ í‰ê°€ë¥¼ ìˆ˜í–‰í–ˆì„ ë•Œ ë” ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, â€˜Quora Question Pairsâ€™ ì‘ì—…ìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ì´ â€˜MRPCâ€™(ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ë¬¸ì¥ì´ ì„œë¡œ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ í‘œí˜„ë˜ì—ˆëŠ”ì§€ íŒë³„í•˜ëŠ” ì‘ì—…) ì‘ì—…ì—ì„œë„ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.</p>

<p>ì´ëŸ¬í•œ ê²°ê³¼ëŠ” ì†Œí”„íŠ¸ í”„ë¡¬í”„íŠ¸ íŠœë‹ì´ ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê³ , íŠ¹ì • ë„ë©”ì¸ì— ê³¼ë„í•˜ê²Œ ìµœì í™”ë˜ì§€ ì•Šë„ë¡ í•˜ëŠ”ë° ë„ì›€ì„ ì¤€ë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ì–¸ì–´ ëª¨ë¸ì„ ë‹¤ì–‘í•œ ì‘ì—…ì— ì ìš©í•˜ê³ ì í•  ë•Œ í”„ë¡¬í”„íŠ¸ íŠœë‹ì€ ë§¤ìš° ìœ ìš©í•œ ë„êµ¬ê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<p>ë” ìì„¸í•œ ì •ë³´ì™€ ì—°êµ¬ ê²°ê³¼ëŠ” <a href="https://blog.research.google/2022/02/guiding-frozen-language-models-with.html">Guiding Frozen Language Models with Learned Soft Prompts</a>ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.</p>

<h1 id="reference">
<a class="anchor" href="#reference" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reference</h1>

<p><a href="https://arxiv.org/pdf/2104.08691.pdf">https://arxiv.org/pdf/2104.08691.pdf</a></p>

<p><a href="https://4n3mone.tistory.com/7">https://4n3mone.tistory.com/7</a></p>


<div class="about">
<div class="about__divider">*****</div>
<div class="about__text">
<br>â˜• Pudhina theme by Knhash ğŸ› ï¸
</div>
</div>


        </div>
        <script src="/assets/vendor/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
    </body>
</html>